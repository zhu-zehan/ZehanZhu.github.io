<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<link rel="stylesheet" href="example.css" type="text/css" />
<title>Introduction to the summer school</title>
<!-- MathJax -->
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async>
</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
	  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
<!-- End MathJax -->
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Additional Features</div>
<div class="menu-item"><a href="mathjax.html">MathJax</a></div>
<div class="menu-item"><a href="underscore.html">Underscore</a></div>
<div class="menu-item"><a href="link.html">Link</a></div>
<div class="menu-item"><a href="http://www.google.com" target="blank">Open&nbsp;in&nbsp;New&nbsp;Tab</a></div>
<div class="menu-item"><a href="http://www.google.com" target="blank">ZhuZehan</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Introduction to the summer school</h1>
<div id="subtitle">The summer school covers several recent advances in the topic of distributed control, optimization and learning. The main object of the course is to introduce advanced control and optimization methods for large-scale systems that arise in modern control engineering and data science. The content of the course covering basics for convex optimization, first-order optimization methods (e.g., (proximal) gradient method, accelerated gradient methods and primal-dual methods), decomposition and splitting methods (e.g., dual decomposition, monotone operators and operator splitting), as well as recent developed parallel and distributed algorithms. To enhance the learning outcome of students, new online learning models, such as SPOC, will be likely employed to promote self-learning and diversity of learning processes, along with a bunch of concrete examples including smart grid, sensor networks and machine learning for the sake of enriching the content of the course. After completing this course, the students are expected to be able to apply the control and optimization techniques learned from this course to large-scale cyber-physical systems as well as other related research areas.

</div>
</div>
<h1>Course Content</h1>
<p>Day 1. Introduction to the course
</p>
<ul>
<li><p>Background • Graph Basics and Weight Matrix • (Dynamic) Average Consensus \n


Day 2. Basics of Convex Optimization
</p>
</li>
<li><p>Convex Set • (Non)-Convex Function • Smoothness • Strong Convexity • Strong Duality • Slater condition • KKT
</p>
</li>
<li><p>(Unconstrained) Convex Optimization Problem • Gradient Method (Constant Stepsize and Derivation from MM perspective) • Convergence Property


Day 3. Stochastic Optimization
</p>
</li>
<li><p>Stochastic Gradient
</p>
</li>
<li><p>Variance Reduction: SAG, SAGA, SVRG…


Day 4. Distributed Convex Optimization
</p>
</li>
<li><p>Finite Sum Problem (some examples) • Distributed Optimization
</p>
</li>
<li><p>Distributed Algorithms:
</p>
<ul>
<li><p>DGD,
</p>
</li>
<li><p>Gradient Tracking,
</p>
</li>
<li><p>Push-Pull/SONATA,
</p>
</li>
<li><p>Primal-Dual (Lagrangian perspective)
</p>
</li></ul>
</li>
<li><p>Connection to Variance Reduction


Day 5. Advanced Topics
</p>
</li>
<li><p>Composite Optimization • Proximal/Projected gradient;
</p>
</li>
<li><p>Primal-Dual Methods：EXTRA, D-ADMM and many others
</p>
</li>
<li><p>Proximal Point Method • Monotone Operators • ADMM

</p>
</li>
</ul>
<h1>Way of Teaching</h1>
<ul>
<li><p>Online (Key idea; Logical flow) + offline (Q&amp;A; mathematical derivation);
</p>
</li>
<li><p>Lectures (focused on optimization) + Seminars (covering topics on control, optimization and learning)

</p>
</li>
</ul>
<h1>Invited speakers</h1>
<ul>
<li><p>Prof Gesualdo Scutari, Purdue University
</p>
</li>
<li><p>Prof Ying Sun, Pennsylvania State University
</p>
</li>
<li><p>Prof Lihua Xie, Nanyang Technological University
</p>
</li>
<li><p>Prof Hoi To Wai, Chinese University of Hong Kong
</p>
</li>
<li><p>Prof Na Li, Harvard Unviersity (TBC)
</p>
</li>
<li><p>Prof Angelia Nedich, Arizona State University (TBC)
</p>
</li>
<li><p>Prof Usman Khan, Tufts University(TBC)
</p>
</li>
<li><p>Tao Lin, EPFL (TBD)
</p>
</li>
<li><p>Dr. Kun Yuan, Damo Academy
</p>
</li>
<li><p>Dr. Huan Li, Nankai University (TBI)

</p>
</li>
</ul>
<h1>Teaching</h1>
<p>Instructor for:
</p>
<ul>
<li><p>Introduction to Linear Dynamical Systems
(<a href="http://www.stanford.edu/class/ee263s/" target=&ldquo;blank&rdquo;>EE263s</a>), Stanford University, Summer 2009
</p>
</li>
<li><p><a href="http://www.kuleuven.be/optec/athens/2010" target=&ldquo;blank&rdquo;>Embedded and Convex Optimization for
Control</a>, one week course, KU Leuven, Belgium, March 2010
<br />



Teaching assistant at Stanford University for:
</p>
</li>
<li><p>Introduction to Linear Dynamical Systems
(<a href="http://www.stanford.edu/class/ee263/" target=&ldquo;blank&rdquo;>EE263</a>), Autumn 2006, 2007 and 2008
</p>
</li>
<li><p>Linear Dynamical Systems (<a href="http://www.stanford.edu/class/ee363/" target=&ldquo;blank&rdquo;>EE363</a>),
Winter 2009
</p>
</li>
<li><p>Convex Optimization I (<a href="http://www.stanford.edu/class/ee364a/" target=&ldquo;blank&rdquo;>EE364a</a>),
Winter 2008
</p>
</li>
<li><p>Convex Optimization II (<a href="http://www.stanford.edu/class/ee364b/" target=&ldquo;blank&rdquo;>EE364b</a>),
Winter 2007 and Spring 2008
</p>
</li>
</ul>
<div id="footer">
<div id="footer-text">
Page generated 2021-07-06 21:10:58 CST, by <a href="https://github.com/wsshin/jemdoc_mathjax" target="blank">jemdoc+MathJax</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
